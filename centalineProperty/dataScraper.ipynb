{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the basic info\n",
    "\n",
    "Though the URL didn't change when we go to next page, by Network Trace Analysis we could see the Request URL that in json format. And we could go to every page when the page number added to the last of template URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info from: http://hk.centanet.com/findproperty/zh-HK/Home/SearchResult?posttype=S&src=D\n",
    "import urllib.request\n",
    "import json\n",
    "URL = 'http://hk.centanet.com/findproperty/BLL/Result_SearchHandler.ashx?url=http%3A%2F%2Fhk.centanet.com%2Ffindproperty%2Fzh-HK%2FHome%2FSearchResult%3Fposttype%3DS%26src%3DF%26minprice%3D%26maxprice%3D%26sortcolumn%3D%26sorttype%3D%26limit%3D-1%26currentpage%3D'\n",
    "\n",
    "for i in range(1,15891):\n",
    "    with urllib.request.urlopen(f'{URL}{i}') as url:\n",
    "        data = json.loads(url.read().decode()).get('post')\n",
    "    print(f'page{i}_len : ', len(data))\n",
    "    text_file = open(f'page{i}.html', \"w\")\n",
    "    text_file.write(data)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract infomation from scraped pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "import pandas as pd\n",
    "\n",
    "base = 'page'\n",
    "suffix = '.html'\n",
    "\n",
    "postid = []\n",
    "locations = []\n",
    "buildings = []\n",
    "prices = []\n",
    "rooms = []\n",
    "usage = [] #實用率\n",
    "\n",
    "\n",
    "for i in range(1,15891):\n",
    "        url = f'{base}{i}{suffix}'\n",
    "        #print(url)\n",
    "        f = open(url)\n",
    "        mypages = BeautifulSoup(f, 'html.parser')\n",
    "        #print(mypages)\n",
    "        for infos in mypages.find_all('div', attrs={'class': 'SearchResult_Row'}):               \n",
    "                postid.append(infos['postid']) \n",
    "                for info in infos.find_all('div', attrs = {'class':'ContentInfo_Left fLeft'}):                                             \n",
    "                    location = info.find('p').text\n",
    "                    locations.append(location)\n",
    "                    buildings.append(info.find('div', attrs = {'class':'ContentInfo_HeaderWrapper'}).text.strip())\n",
    "                    prices.append(info.find('div', attrs = {'class':'price_size'}).text.strip().replace(',',\"\").replace('\\n\\n\\n',\",\").replace('\\n',\",\"))\n",
    "                    rooms.append(info.find('p', attrs = {'class':'ContentInfo_DetailStr_Lf fLeft OneRow'}))\n",
    "                    usage.append(info.find_all('p', attrs = {'class':'ContentInfo_DetailStr_Lf leftBorder fLeft OneRow'}))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "        \"ids\": postid,\n",
    "        \"locations\": locations, \n",
    "        \"buildings\": buildings, \n",
    "        \"prices\": prices,\n",
    "        \"rooms\": rooms,\n",
    "        \"usage\":usage\n",
    "       \n",
    "    })\n",
    "print(data)\n",
    "\n",
    "data.to_csv(\"pages.csv\",encoding='utf_8_sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of schools\n",
    "\n",
    "Using Network Trace Analysis we could see the number of schools also in json format, but the trick part here is the url is gerenated by the buildings `code`, and up to now we only have the `postid`. So I use the postid to go to every page and find the code one by one. \n",
    "\n",
    "Another question is one building may have serverl different `code`. It is because Centaline Property may named the code for different scope.  \n",
    "\n",
    "So while I link the postid and code first, then link the code and numberOfSchools, after that group the code and numberOfSchools by postid, then sort every little group and droped duplicates to get the infomation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('pages.csv')\n",
    "postid = df['ids']\n",
    "\n",
    "URL = 'http://hk.centanet.com/findproperty/zh-HK/Home/Detail/?ID={}&src=S'\n",
    "urls = []\n",
    "links = []\n",
    "codeLink = []\n",
    "codes = []\n",
    "\n",
    "\n",
    "# \n",
    "for i in range(len(postid)): \n",
    "    urls.append(URL.format(postid[i])) \n",
    "\n",
    "links = []\n",
    "codeLink = []\n",
    "codes = []\n",
    "postidRE = []\n",
    "for url in urls:\n",
    "    source = requests.get(url)\n",
    "    soup = BeautifulSoup(source.content, features='html.parser')\n",
    "    for nav in soup.find_all('div', attrs = {'class':'navigationBar fLeft'}):\n",
    "        for link in nav.find_all('a'):\n",
    "            if re.search('code=', link.get('href')):\n",
    "                code = link.get('href')[re.search('code=', link.get('href')).end():]\n",
    "                codes.append(code)  \n",
    "                postidRE.append(re.search(r'ID=(.+)&', url).group(1) ) \n",
    "\n",
    "\n",
    "data = pd.DataFrame({\n",
    "        \"postidRE\": postidRE,\n",
    "        \"code\": codes      \n",
    "    })\n",
    "data\n",
    "# we could observer the school type here: kindergarten: 1, pre-school:2, middle-school:3, international-school:4\n",
    "schoolURL = 'http://hk.centanet.com/findproperty/BLL/Detail_SchoolHandler.ashx?schooltype={}&cestcode={}&currentpage=1'\n",
    "\n",
    "schools1 = []\n",
    "schools2 = []\n",
    "schools3 = []\n",
    "schools4 = []\n",
    "for i in range(len(codes)):\n",
    "    schools1.append(schoolURL.format('1',codes[i])) \n",
    "    schools2.append(schoolURL.format('2',codes[i]))\n",
    "    schools3.append(schoolURL.format('3',codes[i]))\n",
    "    schools4.append(schoolURL.format('4',codes[i]))\n",
    "\n",
    "# repeat for school 1 - 4\n",
    "school1Codes = []\n",
    "totalNumber1 = []\n",
    "for i in range(len(schools1)):\n",
    "    with urllib.request.urlopen(schools1[i]) as url:  \n",
    "#         print(url)\n",
    "        data = json.loads(url.read().decode())\n",
    "        page = data.get('total')\n",
    "        #print(page)\n",
    "        if page == '0':\n",
    "            pass\n",
    "        else:\n",
    "            school1Codes.append(re.search(r'code=(\\w+)', schools1[i]).group(1)) \n",
    "            totalNumber1.append(page)\n",
    "          \n",
    "    \n",
    "    \n",
    "    data1 = pd.DataFrame({\n",
    "        \"code\": school1Codes,\n",
    "        \"totalNumber1\": totalNumber1\n",
    "       \n",
    "    })\n",
    "#merge the datatrame with postid and code, code and numberOfSchools together, groupby postid, sort within groups, drop duplicates. \n",
    "\n",
    "#repeat for school 1-4\n",
    "result1 = pd.merge(data, data1, how='left',on = 'code')\n",
    "result11 = result1.groupby(['postidRE'], sort = False).apply(lambda x: x.sort_values('totalNumber1', ascending = False))\n",
    "result_1 = result11.drop_duplicates(subset='postidRE',keep='first')\n",
    "\n",
    "df0 = df.rename(index=str, columns={\"ids\": \"postidRE\"})\n",
    "df1 = pd.merge(df0, result_1[['postidRE','totalNumber1']], how='left', on = 'postidRE')\n",
    "\n",
    "\n",
    "df4.to_csv('withSchools.csv', encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
